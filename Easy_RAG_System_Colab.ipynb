{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83e\udd16 Easy RAG System for Google Colab\n",
        "\n",
        "This notebook creates a simple but powerful Question Answering system using Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "**What you'll learn:**\n",
        "- How RAG systems work\n",
        "- Building a document knowledge base\n",
        "- Semantic search with vector embeddings\n",
        "- Generating contextual answers\n",
        "\n",
        "**No complex setup required - everything runs in Colab!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install -q faiss-cpu sentence-transformers gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"\u2705 Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfd7\ufe0f Building the RAG System\n",
        "\n",
        "Our RAG system has 3 main components:\n",
        "1. **Document Store**: Where we keep our knowledge base\n",
        "2. **Retriever**: Finds relevant documents for a question\n",
        "3. **Generator**: Creates answers using retrieved context"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EasyRAG:\n",
        "    \"\"\"Simple RAG system for beginners\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        print(\"\ud83d\ude80 Initializing RAG System...\")\n",
        "        \n",
        "        # Load embedding model\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"\ud83d\udcca Embedding model loaded\")\n",
        "        \n",
        "        # Create vector database (FAISS)\n",
        "        self.embedding_dim = 384  # Dimension for all-MiniLM-L6-v2\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        print(\"\ud83d\uddc3\ufe0f Vector database initialized\")\n",
        "        \n",
        "        # Storage for documents\n",
        "        self.documents = []\n",
        "        \n",
        "        print(\"\u2705 RAG System ready!\")\n",
        "    \n",
        "    def add_documents(self, texts: List[str]):\n",
        "        \"\"\"Add documents to the knowledge base\"\"\"\n",
        "        print(f\"\ud83d\udcda Adding {len(texts)} documents...\")\n",
        "        \n",
        "        # Create embeddings for all documents\n",
        "        embeddings = self.embedder.encode(texts, show_progress_bar=True)\n",
        "        \n",
        "        # Add embeddings to FAISS index\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "        \n",
        "        # Store the original texts\n",
        "        self.documents.extend(texts)\n",
        "        \n",
        "        print(f\"\u2705 Total documents in knowledge base: {len(self.documents)}\")\n",
        "    \n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Search for relevant documents\"\"\"\n",
        "        if not self.documents:\n",
        "            return []\n",
        "        \n",
        "        # Convert query to embedding\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        \n",
        "        # Search in FAISS index\n",
        "        distances, indices = self.index.search(\n",
        "            query_embedding.astype('float32'), \n",
        "            min(top_k, len(self.documents))\n",
        "        )\n",
        "        \n",
        "        # Prepare results\n",
        "        results = []\n",
        "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "            if idx < len(self.documents):\n",
        "                similarity = 1 / (1 + distance)  # Convert distance to similarity\n",
        "                results.append({\n",
        "                    \"text\": self.documents[idx],\n",
        "                    \"similarity\": similarity,\n",
        "                    \"rank\": i + 1\n",
        "                })\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def answer_question(self, question: str) -> tuple:\n",
        "        \"\"\"Answer a question using retrieval-augmented generation\"\"\"\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        relevant_docs = self.search(question, top_k=3)\n",
        "        \n",
        "        if not relevant_docs:\n",
        "            return \"I don't have information to answer this question.\", []\n",
        "        \n",
        "        # Step 2: Create context from retrieved documents\n",
        "        context = \"\\n\\n\".join([doc[\"text\"] for doc in relevant_docs])\n",
        "        \n",
        "        # Step 3: Generate answer (simple extractive approach)\n",
        "        # For this simple version, we'll return the most relevant context\n",
        "        answer = f\"Based on the available information:\\n\\n{relevant_docs[0]['text']}\"\n",
        "        \n",
        "        if len(relevant_docs) > 1:\n",
        "            answer += f\"\\n\\nAdditional relevant information:\\n{relevant_docs[1]['text'][:200]}...\"\n",
        "        \n",
        "        return answer, relevant_docs\n",
        "\n",
        "print(\"\ud83c\udfaf RAG system class defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcda Creating a Sample Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sample documents for our knowledge base\n",
        "sample_documents = [\n",
        "    \"Artificial Intelligence (AI) is the simulation of human intelligence in machines that are programmed to think and learn like humans. It includes machine learning, natural language processing, and computer vision.\",\n",
        "    \n",
        "    \"Machine Learning is a subset of AI that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on developing computer programs that can access data and use it to learn.\",\n",
        "    \n",
        "    \"Deep Learning is a subset of machine learning that uses neural networks with multiple layers to model and understand complex patterns in data. It has been particularly successful in image recognition and natural language processing.\",\n",
        "    \n",
        "    \"Python is a popular programming language for AI and machine learning due to its simplicity, extensive libraries like TensorFlow, PyTorch, and scikit-learn, and strong community support.\",\n",
        "    \n",
        "    \"Google Colab provides free access to computing resources including GPUs, making it an excellent platform for machine learning experiments and learning AI concepts without expensive hardware.\",\n",
        "    \n",
        "    \"RAG (Retrieval-Augmented Generation) systems combine information retrieval with text generation to produce more accurate and contextual responses by first finding relevant information from a knowledge base.\",\n",
        "    \n",
        "    \"Vector databases store high-dimensional vectors (embeddings) and provide fast similarity search capabilities, which are essential for modern AI applications like recommendation systems and semantic search.\",\n",
        "    \n",
        "    \"FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors, developed by Facebook Research for handling large-scale vector databases.\",\n",
        "    \n",
        "    \"Sentence Transformers generate dense vector representations of sentences that capture their semantic meaning, enabling comparison of text similarity beyond simple keyword matching.\",\n",
        "    \n",
        "    \"Natural Language Processing (NLP) is a branch of AI that helps computers understand, interpret, and manipulate human language, including tasks like translation, sentiment analysis, and question answering.\"\n",
        "]\n",
        "\n",
        "print(f\"\ud83d\udcd6 Created {len(sample_documents)} sample documents\")\n",
        "print(\"\\nFirst document preview:\")\n",
        "print(sample_documents[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfac Initialize and Test the RAG System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create our RAG system\n",
        "rag = EasyRAG()\n",
        "\n",
        "# Add documents to the knowledge base\n",
        "rag.add_documents(sample_documents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the system with a sample question\n",
        "test_question = \"What is machine learning?\"\n",
        "print(f\"\ud83e\udd14 Question: {test_question}\")\n",
        "\n",
        "answer, retrieved_docs = rag.answer_question(test_question)\n",
        "\n",
        "print(f\"\\n\ud83e\udd16 Answer:\\n{answer}\")\n",
        "\n",
        "print(f\"\\n\ud83d\udd0d Retrieved {len(retrieved_docs)} relevant documents:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"{i+1}. Similarity: {doc['similarity']:.3f}\")\n",
        "    print(f\"   Text: {doc['text'][:100]}...\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfa8 Interactive Web Interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_interface(question):\n",
        "    \"\"\"Gradio interface function\"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question!\", \"No documents retrieved.\"\n",
        "    \n",
        "    answer, docs = rag.answer_question(question)\n",
        "    \n",
        "    # Format retrieved documents for display\n",
        "    docs_display = \"\\n\\n\".join([\n",
        "        f\"\ud83d\udcc4 Document {i+1} (Similarity: {doc['similarity']:.3f}):\\n{doc['text']}\"\n",
        "        for i, doc in enumerate(docs)\n",
        "    ])\n",
        "    \n",
        "    return answer, docs_display\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Ask a Question\",\n",
        "        placeholder=\"What would you like to know?\",\n",
        "        lines=2\n",
        "    ),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\", lines=6),\n",
        "        gr.Textbox(label=\"Retrieved Documents\", lines=8)\n",
        "    ],\n",
        "    title=\"\ud83e\udd16 Easy RAG System Demo\",\n",
        "    description=\"Ask questions about AI, machine learning, and related topics!\",\n",
        "    examples=[\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"How does deep learning work?\",\n",
        "        \"What are the benefits of Python for AI?\",\n",
        "        \"Tell me about Google Colab\",\n",
        "        \"What is a vector database?\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udd27 Advanced Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Function to add custom documents\n",
        "def add_custom_documents(text_input):\n",
        "    \"\"\"Add custom text to the knowledge base\"\"\"\n",
        "    if not text_input.strip():\n",
        "        return \"Please provide some text to add.\"\n",
        "    \n",
        "    # Split by double newlines to separate paragraphs\n",
        "    paragraphs = [p.strip() for p in text_input.split('\\n\\n') if p.strip()]\n",
        "    \n",
        "    if paragraphs:\n",
        "        rag.add_documents(paragraphs)\n",
        "        return f\"\u2705 Added {len(paragraphs)} new document(s) to the knowledge base!\"\n",
        "    else:\n",
        "        return \"No valid paragraphs found in the input.\"\n",
        "\n",
        "# Create interface for adding documents\n",
        "add_docs_demo = gr.Interface(\n",
        "    fn=add_custom_documents,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Add New Documents\",\n",
        "        placeholder=\"Paste your text here. Separate different documents with double newlines.\",\n",
        "        lines=5\n",
        "    ),\n",
        "    outputs=gr.Textbox(label=\"Status\"),\n",
        "    title=\"\ud83d\udcda Add Documents to Knowledge Base\",\n",
        "    description=\"Expand the knowledge base by adding your own documents!\"\n",
        ")\n",
        "\n",
        "add_docs_demo.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udfaf More Examples and Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with multiple questions\n",
        "test_questions = [\n",
        "    \"What is the difference between AI and machine learning?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"Why is Python popular for AI?\",\n",
        "    \"What can I do with Google Colab?\"\n",
        "]\n",
        "\n",
        "print(\"\ud83e\uddea Testing with multiple questions:\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    answer, docs = rag.answer_question(question)\n",
        "    print(f\"Answer: {answer[:200]}...\")\n",
        "    print(f\"Retrieved {len(docs)} documents\\n\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83d\udcca System Statistics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display system information\n",
        "print(\"\ud83d\udcca RAG System Statistics:\")\n",
        "print(f\"\ud83d\udcda Total documents: {len(rag.documents)}\")\n",
        "print(f\"\ud83e\uddee Embedding dimension: {rag.embedding_dim}\")\n",
        "print(f\"\ud83d\uddc3\ufe0f FAISS index size: {rag.index.ntotal}\")\n",
        "print(f\"\ud83d\udd22 Average document length: {np.mean([len(doc) for doc in rag.documents]):.0f} characters\")\n",
        "\n",
        "# Show sample of document lengths\n",
        "doc_lengths = [len(doc) for doc in rag.documents]\n",
        "print(f\"\ud83d\udccf Document length range: {min(doc_lengths)} - {max(doc_lengths)} characters\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## \ud83c\udf89 Congratulations!\n",
        "\n",
        "You've successfully built and tested a RAG system! Here's what you've accomplished:\n",
        "\n",
        "\u2705 **Built a complete RAG pipeline** with document storage, retrieval, and generation\n",
        "\n",
        "\u2705 **Used semantic search** with vector embeddings to find relevant information\n",
        "\n",
        "\u2705 **Created an interactive web interface** for easy testing\n",
        "\n",
        "\u2705 **Learned to add custom documents** to expand the knowledge base\n",
        "\n",
        "## \ud83d\ude80 Next Steps\n",
        "\n",
        "You can enhance this system by:\n",
        "\n",
        "1. **Adding more documents** - Upload text files or paste content\n",
        "2. **Using better language models** - Integrate OpenAI API or local models like Ollama\n",
        "3. **Improving chunking** - Split long documents into smaller, more focused pieces\n",
        "4. **Adding metadata** - Include categories, dates, or sources for better filtering\n",
        "5. **Implementing re-ranking** - Use additional models to improve retrieval quality\n",
        "\n",
        "## \ud83d\udcda Resources for Learning More\n",
        "\n",
        "- [LangChain Documentation](https://docs.langchain.com/)\n",
        "- [Sentence Transformers](https://www.sbert.net/)\n",
        "- [FAISS Documentation](https://faiss.ai/)\n",
        "- [RAG Paper](https://arxiv.org/abs/2005.11401)\n",
        "\n",
        "Happy building! \ud83e\udd16\u2728"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}