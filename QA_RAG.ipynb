{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMucphRDVTznfPbHoowOh6k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c5d16ab5d5d4e7d8f85be16d87183fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed56e1bf6a864685aa6a4994dd01be0a",
              "IPY_MODEL_6c2504dc63d946d798df8f506319f2a8",
              "IPY_MODEL_12cc55546dbb42f2b709dc8a28560b89"
            ],
            "layout": "IPY_MODEL_d3587c47b8d646c7af52c8280fec72dc"
          }
        },
        "ed56e1bf6a864685aa6a4994dd01be0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f5dd8091a5a4502953004bdc61e6393",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9295672a7f684eb9a8c6a945a3c5cd5f",
            "value": "Batches:‚Äá100%"
          }
        },
        "6c2504dc63d946d798df8f506319f2a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865a004c7e7249a39f08ff869f24b364",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb02b577a9074086a26d877efbb8da4d",
            "value": 2
          }
        },
        "12cc55546dbb42f2b709dc8a28560b89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcf625d5edee46fda71c4f7cd6046d73",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5de324ff88b74b728038f6a2afabce9f",
            "value": "‚Äá2/2‚Äá[00:02&lt;00:00,‚Äá‚Äá1.12it/s]"
          }
        },
        "d3587c47b8d646c7af52c8280fec72dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f5dd8091a5a4502953004bdc61e6393": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9295672a7f684eb9a8c6a945a3c5cd5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "865a004c7e7249a39f08ff869f24b364": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb02b577a9074086a26d877efbb8da4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcf625d5edee46fda71c4f7cd6046d73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de324ff88b74b728038f6a2afabce9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divyansh-dhawan/Question-Answering-System-using-RAG/blob/main/QA_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install -q faiss-cpu sentence-transformers gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHtjDFvzlCM5",
        "outputId": "39d37941-ecba-494f-ac89-1e7eee6e04f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import gradio as gr\n",
        "from typing import List, Dict\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "97wS8tBAlPG0",
        "outputId": "6d5fb7d9-aa68-41fe-a72a-3fadcb5803c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RAG:\n",
        "    \"\"\"RAG system\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        print(\"üöÄ Initializing RAG System...\")\n",
        "\n",
        "        # Load embedding model\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"üìä Embedding model loaded\")\n",
        "\n",
        "        # Create vector database (FAISS)\n",
        "        self.embedding_dim = 384  # Dimension for all-MiniLM-L6-v2\n",
        "        self.index = faiss.IndexFlatL2(self.embedding_dim)\n",
        "        print(\"üóÉÔ∏è Vector database initialized\")\n",
        "\n",
        "        # Storage for documents\n",
        "        self.documents = []\n",
        "\n",
        "        print(\"‚úÖ RAG System ready!\")\n",
        "\n",
        "    def add_documents(self, texts: List[str]):\n",
        "        \"\"\"Add documents to the knowledge base\"\"\"\n",
        "        print(f\"üìö Adding {len(texts)} documents...\")\n",
        "\n",
        "        # Create embeddings for all documents\n",
        "        embeddings = self.embedder.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        # Add embeddings to FAISS index\n",
        "        self.index.add(embeddings.astype('float32'))\n",
        "\n",
        "        # Store the original texts\n",
        "        self.documents.extend(texts)\n",
        "\n",
        "        print(f\"‚úÖ Total documents in knowledge base: {len(self.documents)}\")\n",
        "\n",
        "    def search(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Search for relevant documents\"\"\"\n",
        "        if not self.documents:\n",
        "            return []\n",
        "\n",
        "        # Convert query to embedding\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "\n",
        "        # Search in FAISS index\n",
        "        distances, indices = self.index.search(\n",
        "            query_embedding.astype('float32'),\n",
        "            min(top_k, len(self.documents))\n",
        "        )\n",
        "\n",
        "        # Prepare results\n",
        "        results = []\n",
        "        for i, (distance, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "            if idx < len(self.documents):\n",
        "                similarity = 1 / (1 + distance)  # Convert distance to similarity\n",
        "                results.append({\n",
        "                    \"text\": self.documents[idx],\n",
        "                    \"similarity\": similarity,\n",
        "                    \"rank\": i + 1\n",
        "                })\n",
        "\n",
        "        return results\n",
        "\n",
        "    def answer_question(self, question: str) -> tuple:\n",
        "        \"\"\"Answer a question using retrieval-augmented generation\"\"\"\n",
        "        # Step 1: Retrieve relevant documents\n",
        "        relevant_docs = self.search(question, top_k=3)\n",
        "\n",
        "        if not relevant_docs:\n",
        "            return \"I don't have information to answer this question.\", []\n",
        "\n",
        "        # Step 2: Create context from retrieved documents\n",
        "        context = \"\\n\\n\".join([doc[\"text\"] for doc in relevant_docs])\n",
        "\n",
        "        # Step 3: Generate answer (simple extractive approach)\n",
        "        # For this simple version, we'll return the most relevant context\n",
        "        answer = f\"Based on the available information:\\n\\n{relevant_docs[0]['text']}\"\n",
        "\n",
        "        if len(relevant_docs) > 1:\n",
        "            answer += f\"\\n\\nAdditional relevant information:\\n{relevant_docs[1]['text'][:200]}...\"\n",
        "\n",
        "        return answer, relevant_docs\n",
        "\n",
        "print(\"üéØ RAG system class defined!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIF6Rrsrlyo7",
        "outputId": "45cebff9-bfa2-4dc3-fb6e-c0b71e7fedf1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üéØ RAG system class defined!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents for our knowledge base\n",
        "sample_documents = [\n",
        "\n",
        "    # Neural Networks\n",
        "    \"Neural networks are machine learning models inspired by the structure and function of the human brain. They consist of layers of artificial neurons organized into an input layer, one or more hidden layers, and an output layer. Each neuron processes input data and passes the information forward through weighted connections and non-linear activation functions like ReLU or Sigmoid. Neural networks are trained using supervised learning techniques such as backpropagation and gradient descent to minimize a loss function. These models are powerful for recognizing patterns and approximating complex non-linear functions.\",\n",
        "    \"The architecture of a neural network includes neurons (also called nodes), weights, biases, and activation functions. Training involves feeding labeled data through the network, calculating the loss, and updating the weights using optimization algorithms. Common challenges include overfitting, vanishing gradients, and the need for large datasets.\",\n",
        "    \"Neural networks are used in a wide range of applications including image and speech recognition, financial forecasting, medical diagnosis, and time series prediction. They serve as the foundational architecture for many deep learning models and are widely implemented in fields like computer vision, natural language processing, and autonomous systems.\",\n",
        "\n",
        "    # Convolutional Neural Networks (CNNs)\n",
        "    \"Convolutional Neural Networks (CNNs) are a class of deep learning models particularly effective at processing spatial and visual data such as images and videos. They apply convolutional filters to local receptive fields in the input to capture hierarchical patterns and features. CNNs use layers like convolution, activation (typically ReLU), pooling (like max pooling), and fully connected layers to perform classification or regression tasks.\",\n",
        "    \"A standard CNN architecture consists of several convolutional layers for feature extraction, pooling layers for dimensionality reduction, and fully connected layers for classification. Techniques like dropout and batch normalization are used for regularization and performance improvement. Famous CNN architectures include LeNet-5, AlexNet, VGGNet, and ResNet.\",\n",
        "    \"CNNs are widely used in applications such as image classification, object detection (YOLO, SSD), semantic segmentation, facial recognition, and even video analysis. They are also adapted for non-image tasks involving spatial hierarchies like audio signal processing and text classification. Despite their power, CNNs require significant computational resources and large labeled datasets for training.\",\n",
        "\n",
        "    # Transfer Learning\n",
        "    \"Transfer learning is a deep learning technique that involves reusing a pre-trained model on a new, related task. Instead of training a model from scratch, transfer learning leverages knowledge gained from a large source dataset and adapts it to a target task with less data. This is especially useful when labeled data is limited or expensive to obtain.\",\n",
        "    \"There are two main approaches to transfer learning: feature extraction and fine-tuning. In feature extraction, a pre-trained model is used to extract features from new data, and a new classifier is trained on top. In fine-tuning, the weights of the pre-trained model are further trained on the new dataset, often with a reduced learning rate. Popular pre-trained models include ResNet for images and BERT or GPT for text.\",\n",
        "    \"Transfer learning is widely used in computer vision (e.g., using ImageNet-pretrained CNNs for medical imaging), natural language processing (e.g., fine-tuning BERT for sentiment analysis), and audio recognition. It offers benefits such as reduced training time, improved accuracy on small datasets, and better generalization. However, it may suffer from domain mismatch and inherited biases from the source data.\",\n",
        "\n",
        "    # Recurrent Neural Networks (RNNs)\n",
        "    \"Recurrent Neural Networks (RNNs) are a type of neural network designed for sequential and time-series data. Unlike feedforward networks, RNNs have connections that form cycles, allowing them to maintain an internal memory of previous inputs. This makes them suitable for tasks where the order of input data matters, such as natural language, speech, or sensor data.\",\n",
        "    \"The architecture of an RNN includes an input layer, a hidden recurrent layer, and an output layer. At each time step, the hidden state is updated using both the current input and the previous hidden state, allowing information to persist across the sequence. Common activation functions include Tanh and ReLU.\",\n",
        "    \"RNNs are commonly applied in language modeling, text generation, speech recognition, time series forecasting, and sentiment analysis. However, they suffer from issues like vanishing and exploding gradients, making it hard to learn long-term dependencies in long sequences. This limitation led to the development of advanced architectures like LSTM and GRU.\",\n",
        "\n",
        "    # Long Short-Term Memory (LSTM)\n",
        "    \"Long Short-Term Memory (LSTM) networks are an extension of RNNs designed to capture long-range dependencies in sequential data. They solve the vanishing gradient problem by introducing a more complex architecture with internal memory cells and gates that control the flow of information.\",\n",
        "    \"An LSTM unit includes three main gates: the input gate (controls what new information is stored), the forget gate (controls what information is discarded), and the output gate (controls what is passed to the next step). These gates allow LSTMs to selectively remember or forget information across long sequences.\",\n",
        "    \"LSTMs are widely used in natural language processing tasks such as machine translation, text summarization, and question answering. They are also effective in speech recognition, handwriting generation, and anomaly detection in time series data. While LSTMs improve upon basic RNNs, they are more computationally intensive and slower to train.\",\n",
        "\n",
        "    # Transformer\n",
        "    \"Transformers are a deep learning architecture that processes sequences using self-attention mechanisms, enabling the model to consider all positions in a sequence simultaneously. Unlike RNNs and LSTMs, transformers do not require sequential processing, making them more parallelizable and efficient for large-scale data.\",\n",
        "    \"The core of the transformer architecture is the self-attention mechanism, which allows the model to weigh the importance of each word in a sequence relative to others. Transformers consist of encoder and decoder blocks, each composed of multi-head attention layers, feedforward layers, layer normalization, and residual connections.\",\n",
        "    \"Transformers are the foundation for many state-of-the-art models in natural language processing, including BERT, GPT, and T5. They are used for tasks like machine translation, text generation, summarization, question answering, and code generation. Recent research has also extended transformers to other domains such as image classification (Vision Transformers) and protein folding (AlphaFold). Despite their power, transformers require significant memory and compute resources.\",\n",
        "\n",
        "    # Autoencoders\n",
        "    \"Autoencoders are unsupervised neural networks used to learn efficient representations (encodings) of data. They consist of two main parts: an encoder that compresses the input into a lower-dimensional latent space, and a decoder that reconstructs the original input from this representation. The goal is to minimize the reconstruction error between the input and its output.\",\n",
        "    \"The architecture of an autoencoder includes an input layer, one or more hidden layers forming the encoder, a latent space (bottleneck), and a mirrored set of hidden layers forming the decoder. Activation functions like ReLU, Sigmoid, or Tanh are commonly used, and Mean Squared Error is a typical loss function.\",\n",
        "    \"Autoencoders are used in dimensionality reduction, image denoising, anomaly detection, and feature extraction. Variants include Denoising Autoencoders (for noisy input), Sparse Autoencoders (for sparse representation), and Variational Autoencoders (for generative modeling). However, autoencoders can overfit and may simply learn to copy input unless regularized appropriately.\",\n",
        "\n",
        "    # Large Language Models (LLMs)\n",
        "    \"Large Language Models (LLMs) are deep learning models trained on massive text corpora to understand, process, and generate human-like language. Built on transformer architectures, these models learn rich representations of linguistic structure and semantics, allowing them to perform a wide range of NLP tasks with minimal fine-tuning.\",\n",
        "    \"LLMs consist of millions to billions of parameters and are usually pretrained on general-purpose datasets using objectives like masked language modeling (as in BERT) or autoregressive prediction (as in GPT). After pretraining, they can be fine-tuned or prompted for specific downstream tasks.\",\n",
        "    \"Applications of LLMs include text generation, summarization, translation, sentiment analysis, question answering, chatbot development, and even code generation. Popular LLMs include OpenAI's GPT series, Google's PaLM, Meta's LLaMA, and BERT. These models have sparked a revolution in generative AI, but raise concerns around biases, hallucinations, and computational cost.\",\n",
        "\n",
        "    # Retrieval-Augmented Generation (RAG)\n",
        "    \"Retrieval-Augmented Generation (RAG) is a hybrid approach in which a large language model is augmented with an information retrieval module. Instead of relying solely on internal knowledge, the model retrieves relevant documents from an external source (e.g., a search index or database) and uses that information to generate more accurate and up-to-date responses.\",\n",
        "    \"A RAG system typically consists of two components: a retriever (like BM25 or dense vector-based models) that fetches relevant documents based on the user query, and a generator (like GPT or BART) that conditions on the retrieved context to generate a response. This framework allows open-domain question answering and long-context generation without needing to retrain the language model.\",\n",
        "    \"RAG is used in applications such as knowledge-intensive Q&A systems, personalized assistants, legal or scientific document summarization, and enterprise search. It improves factual consistency and reduces hallucinations by grounding generation in external, authoritative data. However, RAG performance depends heavily on retrieval quality and indexing strategy.\",\n",
        "\n",
        "    # Generative Adversarial Networks (GANs)\n",
        "    \"Generative Adversarial Networks (GANs) are a class of deep learning models that consist of two neural networks‚Äîthe generator and the discriminator‚Äîtrained in opposition. The generator creates synthetic data instances, while the discriminator evaluates whether they are real (from the training data) or fake (from the generator). The two models play a minimax game, where the generator tries to fool the discriminator, and the discriminator tries to correctly identify real versus fake inputs.\",\n",
        "    \"The architecture of a GAN includes: (1) a generator network, typically a deconvolutional neural network, which maps a random noise vector to a synthetic sample (e.g., an image); and (2) a discriminator, typically a CNN, which acts as a binary classifier. The networks are trained iteratively‚Äîfirst updating the discriminator and then the generator.\",\n",
        "    \"GANs are used in realistic image synthesis, super-resolution, image-to-image translation (e.g., turning sketches into photos), text-to-image generation, data augmentation, and video generation. Variants of GANs include Conditional GANs (cGANs), CycleGANs, and StyleGANs. Despite their success, GANs are difficult to train due to mode collapse, instability, and sensitivity to hyperparameters.\",\n",
        "\n",
        "    # Diffusion Models (DDPMs)\n",
        "    \"Denoising Diffusion Probabilistic Models (DDPMs), or simply diffusion models, are a type of generative model that generate data by reversing a diffusion process. The forward process gradually adds noise to training data over a series of steps, and the model learns to reverse this noising process to generate new samples from pure noise.\",\n",
        "    \"A typical diffusion model consists of a U-Net-based neural network trained to predict either the noise added or the original data at each timestep, given a noisy input and a time index. During inference, the model begins with a random noise sample and iteratively denoises it using a learned reverse process to obtain realistic outputs.\",\n",
        "    \"Diffusion models have achieved state-of-the-art results in image generation tasks, rivaling or surpassing GANs in quality and diversity. Applications include high-resolution image synthesis (e.g., DALL¬∑E 2, Stable Diffusion), inpainting, image editing, and even text-to-video generation. They are considered more stable to train than GANs but require longer sampling times during inference.\",\n",
        "\n",
        "    # Explainable AI (XAI)\n",
        "    \"Explainable AI (XAI) refers to techniques and tools that make the behavior and decisions of AI systems interpretable and transparent to humans. XAI is essential for building trust, ensuring fairness, detecting biases, and meeting regulatory requirements‚Äîespecially in high-stakes domains like healthcare, finance, and criminal justice.\",\n",
        "    \"XAI methods can be categorized as intrinsic (models that are inherently interpretable, such as decision trees or linear models) or post-hoc (techniques applied to complex models after training). Post-hoc techniques include SHAP (SHapley Additive exPlanations), LIME (Local Interpretable Model-agnostic Explanations), saliency maps, and counterfactual explanations.\",\n",
        "    \"XAI is used in model debugging, regulatory compliance, fairness audits, and communicating model behavior to non-technical stakeholders. It is especially critical in domains involving accountability and ethical concerns. However, XAI techniques may introduce approximation errors and trade-offs between interpretability and model complexity.\",\n",
        "\n",
        "    # Reinforcement Learning / Deep Reinforcement Learning\n",
        "\"Reinforcement Learning (RL) is a learning paradigm where an agent learns to take actions in an environment to maximize cumulative rewards over time. Unlike supervised learning, RL does not require labeled input/output pairs‚Äîinstead, it relies on feedback in the form of rewards or penalties. Deep Reinforcement Learning (DRL) combines RL with deep neural networks to handle high-dimensional state and action spaces, enabling learning directly from raw inputs like images.\",\n",
        "\"The architecture of RL involves key components: the agent (decision-maker), the environment (where it operates), states (observations), actions, rewards, and a policy (strategy). Deep RL architectures include Deep Q-Networks (DQN), which approximate Q-values using CNNs; Policy Gradient methods that directly optimize the policy; and Actor-Critic models which combine value-based and policy-based approaches. Training involves techniques like Q-learning, REINFORCE, and Advantage Actor-Critic (A2C).\",\n",
        "\"RL/DRL is used in a wide range of applications such as game playing (e.g., AlphaGo, OpenAI Five), robotics (manipulation and navigation), recommendation systems, industrial control, autonomous vehicles, and finance. It enables autonomous systems to improve through experience, but faces challenges like sample inefficiency, exploration-exploitation trade-off, and sensitivity to hyperparameters and reward shaping.\"\n",
        "]\n",
        "\n",
        "print(f\"üìñ Created {len(sample_documents)} sample documents\")\n",
        "print(\"\\nFirst document preview:\")\n",
        "print(sample_documents[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vqlltBIumzTQ",
        "outputId": "3ba3963f-6dab-49b8-cf3c-8ffbd5dec53b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìñ Created 39 sample documents\n",
            "\n",
            "First document preview:\n",
            "Neural networks are machine learning models inspired by the structure and function of the human brain. They consist of layers of artificial neurons organized into an input layer, one or more hidden layers, and an output layer. Each neuron processes input data and passes the information forward through weighted connections and non-linear activation functions like ReLU or Sigmoid. Neural networks are trained using supervised learning techniques such as backpropagation and gradient descent to minimize a loss function. These models are powerful for recognizing patterns and approximating complex non-linear functions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create our RAG system\n",
        "rag = RAG()\n",
        "\n",
        "# Add documents to the knowledge base\n",
        "rag.add_documents(sample_documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153,
          "referenced_widgets": [
            "0c5d16ab5d5d4e7d8f85be16d87183fd",
            "ed56e1bf6a864685aa6a4994dd01be0a",
            "6c2504dc63d946d798df8f506319f2a8",
            "12cc55546dbb42f2b709dc8a28560b89",
            "d3587c47b8d646c7af52c8280fec72dc",
            "8f5dd8091a5a4502953004bdc61e6393",
            "9295672a7f684eb9a8c6a945a3c5cd5f",
            "865a004c7e7249a39f08ff869f24b364",
            "eb02b577a9074086a26d877efbb8da4d",
            "fcf625d5edee46fda71c4f7cd6046d73",
            "5de324ff88b74b728038f6a2afabce9f"
          ]
        },
        "id": "gK6GE6e6m0IT",
        "outputId": "02d03ee1-db5b-4e6a-82b0-48107645fe93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Initializing RAG System...\n",
            "üìä Embedding model loaded\n",
            "üóÉÔ∏è Vector database initialized\n",
            "‚úÖ RAG System ready!\n",
            "üìö Adding 39 documents...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c5d16ab5d5d4e7d8f85be16d87183fd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Total documents in knowledge base: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the system with a sample question\n",
        "test_question = \"What is explainable AI?\"\n",
        "print(f\"ü§î Question: {test_question}\")\n",
        "\n",
        "answer, retrieved_docs = rag.answer_question(test_question)\n",
        "\n",
        "print(f\"\\nü§ñ Answer:\\n{answer}\")\n",
        "\n",
        "print(f\"\\nüîç Retrieved {len(retrieved_docs)} relevant documents:\")\n",
        "for i, doc in enumerate(retrieved_docs):\n",
        "    print(f\"{i+1}. Similarity: {doc['similarity']:.3f}\")\n",
        "    print(f\"   Text: {doc['text'][:100]}...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_Q5yCnQm5cf",
        "outputId": "87846f31-2b3d-4e8f-9456-492e35dc965c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü§î Question: What is explainable AI?\n",
            "\n",
            "ü§ñ Answer:\n",
            "Based on the available information:\n",
            "\n",
            "Explainable AI (XAI) refers to techniques and tools that make the behavior and decisions of AI systems interpretable and transparent to humans. XAI is essential for building trust, ensuring fairness, detecting biases, and meeting regulatory requirements‚Äîespecially in high-stakes domains like healthcare, finance, and criminal justice.\n",
            "\n",
            "Additional relevant information:\n",
            "XAI methods can be categorized as intrinsic (models that are inherently interpretable, such as decision trees or linear models) or post-hoc (techniques applied to complex models after training). Post-...\n",
            "\n",
            "üîç Retrieved 3 relevant documents:\n",
            "1. Similarity: 0.667\n",
            "   Text: Explainable AI (XAI) refers to techniques and tools that make the behavior and decisions of AI syste...\n",
            "\n",
            "2. Similarity: 0.479\n",
            "   Text: XAI methods can be categorized as intrinsic (models that are inherently interpretable, such as decis...\n",
            "\n",
            "3. Similarity: 0.452\n",
            "   Text: XAI is used in model debugging, regulatory compliance, fairness audits, and communicating model beha...\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_interface(question):\n",
        "    \"\"\"Gradio interface function\"\"\"\n",
        "    if not question.strip():\n",
        "        return \"Please enter a question!\", \"No documents retrieved.\"\n",
        "\n",
        "    answer, docs = rag.answer_question(question)\n",
        "\n",
        "    # Format retrieved documents for display\n",
        "    docs_display = \"\\n\\n\".join([\n",
        "        f\"üìÑ Document {i+1} (Similarity: {doc['similarity']:.3f}):\\n{doc['text']}\"\n",
        "        for i, doc in enumerate(docs)\n",
        "    ])\n",
        "\n",
        "    return answer, docs_display\n",
        "\n",
        "# Create Gradio interface\n",
        "demo = gr.Interface(\n",
        "    fn=chat_interface,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Ask a Question\",\n",
        "        placeholder=\"What would you like to know?\",\n",
        "        lines=2\n",
        "    ),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Answer\", lines=6),\n",
        "        gr.Textbox(label=\"Retrieved Documents\", lines=8)\n",
        "    ],\n",
        "    title=\"ü§ñ Easy RAG System Demo\",\n",
        "    description=\"Ask questions about AI, machine learning, and related topics!\",\n",
        "    examples=[\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"How does deep learning work?\",\n",
        "        \"What are the benefits of Python for AI?\",\n",
        "        \"Tell me about Google Colab\",\n",
        "        \"What is a vector database?\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Launch the interface\n",
        "demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "-bgAqasonD6Y",
        "outputId": "f7da79a3-a33f-4c30-be85-eb512741a6ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://89a86551113d6f5e6d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://89a86551113d6f5e6d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to add custom documents\n",
        "def add_custom_documents(text_input):\n",
        "    \"\"\"Add custom text to the knowledge base\"\"\"\n",
        "    if not text_input.strip():\n",
        "        return \"Please provide some text to add.\"\n",
        "\n",
        "    # Split by double newlines to separate paragraphs\n",
        "    paragraphs = [p.strip() for p in text_input.split('\\n\\n') if p.strip()]\n",
        "\n",
        "    if paragraphs:\n",
        "        rag.add_documents(paragraphs)\n",
        "        return f\"‚úÖ Added {len(paragraphs)} new document(s) to the knowledge base!\"\n",
        "    else:\n",
        "        return \"No valid paragraphs found in the input.\"\n",
        "\n",
        "# Create interface for adding documents\n",
        "add_docs_demo = gr.Interface(\n",
        "    fn=add_custom_documents,\n",
        "    inputs=gr.Textbox(\n",
        "        label=\"Add New Documents\",\n",
        "        placeholder=\"Paste your text here. Separate different documents with double newlines.\",\n",
        "        lines=5\n",
        "    ),\n",
        "    outputs=gr.Textbox(label=\"Status\"),\n",
        "    title=\"üìö Add Documents to Knowledge Base\",\n",
        "    description=\"Expand the knowledge base by adding your own documents!\"\n",
        ")\n",
        "\n",
        "add_docs_demo.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "io3zUXFfMiQI",
        "outputId": "4abe01d7-29ba-4e81-f7f1-e2962bd854da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://217d7e0b83d90cf35a.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://217d7e0b83d90cf35a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test with multiple questions\n",
        "test_questions = [\n",
        "    \"What are the applications of CNN?\",\n",
        "    \"How does RAG work?\",\n",
        "    \"What is the difference between RNNs, LSTMs and transformers\",\n",
        "    \"Why is GAN used for generarting?\"\n",
        "]\n",
        "\n",
        "print(\"üß™ Testing with multiple questions:\\n\")\n",
        "\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"Question {i}: {question}\")\n",
        "    answer, docs = rag.answer_question(question)\n",
        "    print(f\"Answer: {answer[:200]}...\")\n",
        "    print(f\"Retrieved {len(docs)} documents\\n\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0yEk7eHM7-v",
        "outputId": "7408e3ae-6ac2-469a-b3e4-40381fd334bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üß™ Testing with multiple questions:\n",
            "\n",
            "Question 1: What are the applications of CNN?\n",
            "Answer: Based on the available information:\n",
            "\n",
            "Convolutional Neural Networks (CNNs) are a class of deep learning models particularly effective at processing spatial and visual data such as images and videos. Th...\n",
            "Retrieved 3 documents\n",
            "\n",
            "--------------------------------------------------\n",
            "Question 2: How does RAG work?\n",
            "Answer: Based on the available information:\n",
            "\n",
            "RAG is used in applications such as knowledge-intensive Q&A systems, personalized assistants, legal or scientific document summarization, and enterprise search. It...\n",
            "Retrieved 3 documents\n",
            "\n",
            "--------------------------------------------------\n",
            "Question 3: What is the difference between RNNs, LSTMs and transformers\n",
            "Answer: Based on the available information:\n",
            "\n",
            "Transformers are a deep learning architecture that processes sequences using self-attention mechanisms, enabling the model to consider all positions in a sequence ...\n",
            "Retrieved 3 documents\n",
            "\n",
            "--------------------------------------------------\n",
            "Question 4: Why is GAN used for generarting?\n",
            "Answer: Based on the available information:\n",
            "\n",
            "GANs are used in realistic image synthesis, super-resolution, image-to-image translation (e.g., turning sketches into photos), text-to-image generation, data augme...\n",
            "Retrieved 3 documents\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display system information\n",
        "print(\"üìä RAG System Statistics:\")\n",
        "print(f\"üìö Total documents: {len(rag.documents)}\")\n",
        "print(f\"üßÆ Embedding dimension: {rag.embedding_dim}\")\n",
        "print(f\"üóÉÔ∏è FAISS index size: {rag.index.ntotal}\")\n",
        "print(f\"üî¢ Average document length: {np.mean([len(doc) for doc in rag.documents]):.0f} characters\")\n",
        "\n",
        "# Show sample of document lengths\n",
        "doc_lengths = [len(doc) for doc in rag.documents]\n",
        "print(f\"üìè Document length range: {min(doc_lengths)} - {max(doc_lengths)} characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWlLFfyDM8mg",
        "outputId": "416c8807-f1cd-4f44-8097-e20c2303ae5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä RAG System Statistics:\n",
            "üìö Total documents: 39\n",
            "üßÆ Embedding dimension: 384\n",
            "üóÉÔ∏è FAISS index size: 39\n",
            "üî¢ Average document length: 377 characters\n",
            "üìè Document length range: 287 - 619 characters\n"
          ]
        }
      ]
    }
  ]
}